{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae34ef8-9168-4b42-84e4-9bd0a4c4474a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Assignment 2: Heart Disease Classification using PySpark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2327f1-1904-4335-b07b-a6481d1e8985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- age: integer (nullable = true)\n |-- sex: integer (nullable = true)\n |-- cp: integer (nullable = true)\n |-- trestbps: integer (nullable = true)\n |-- chol: integer (nullable = true)\n |-- fbs: integer (nullable = true)\n |-- restecg: integer (nullable = true)\n |-- thalach: integer (nullable = true)\n |-- exang: integer (nullable = true)\n |-- oldpeak: double (nullable = true)\n |-- slope: integer (nullable = true)\n |-- ca: integer (nullable = true)\n |-- thal: integer (nullable = true)\n |-- target: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "##  Pre-processing\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"HeartDiseaseClassification\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"/FileStore/tables/heart.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Check schema\n",
    "df.printSchema()\n",
    "\n",
    "# Handle missing values (if any)\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert target column to numeric if needed\n",
    "label_indexer = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ada7cd-1a78-4d65-8215-172c97472523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n|summary|              age|               sex|                cp|          trestbps|             chol|                fbs|           restecg|           thalach|              exang|           oldpeak|             slope|                ca|              thal|            target|\n+-------+-----------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n|  count|             1025|              1025|              1025|              1025|             1025|               1025|              1025|              1025|               1025|              1025|              1025|              1025|              1025|              1025|\n|   mean|54.43414634146342|0.6956097560975609|0.9424390243902439|131.61170731707318|            246.0|0.14926829268292682|0.5297560975609756|149.11414634146342|0.33658536585365856|1.0715121951219524|1.3853658536585365|0.7541463414634146|  2.32390243902439|0.5131707317073171|\n| stddev|9.072290233244278|0.4603733241196495| 1.029640743645865|17.516718005376408|51.59251020618203|0.35652668972715756|0.5278775668748918| 23.00572374597721| 0.4727723760037115|1.1750532551501767|0.6177552671745918|1.0307976650242825|0.6206602380510303|0.5000704980788011|\n|    min|               29|                 0|                 0|                94|              126|                  0|                 0|                71|                  0|               0.0|                 0|                 0|                 0|                 0|\n|    max|               77|                 1|                 3|               200|              564|                  1|                 2|               202|                  1|               6.2|                 2|                 4|                 3|                 1|\n+-------+-----------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n\n+------+-----+\n|target|count|\n+------+-----+\n|     1|  526|\n|     0|  499|\n+------+-----+\n\n+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n| 52|  1|  0|     125| 212|  0|      1|    168|    0|    1.0|    2|  2|   3|     0|\n| 53|  1|  0|     140| 203|  1|      0|    155|    1|    3.1|    0|  0|   3|     0|\n| 70|  1|  0|     145| 174|  0|      1|    125|    1|    2.6|    0|  0|   3|     0|\n| 61|  1|  0|     148| 203|  0|      1|    161|    0|    0.0|    2|  1|   3|     0|\n| 62|  0|  0|     138| 294|  1|      1|    106|    0|    1.9|    1|  3|   2|     0|\n+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "##  Exploratory Data Analysis\n",
    "\n",
    "# Show basic statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Count classes\n",
    "df.groupBy(\"target\").count().show()\n",
    "\n",
    "# Display sample\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197cde06-c738-4909-abd3-f59a23269e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##  Model Building\n",
    "\n",
    "# Features\n",
    "feature_cols = [col for col in df.columns if col != 'target']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# ML model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer, assembler, lr])\n",
    "\n",
    "# Split\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit model\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec3a486-08c7-40b0-b329-29d9e2f943b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9214\nPrecision: 0.7717\nRecall: 0.9103\nF1 Score: 0.8342\nAUC: 0.9214\n"
     ]
    }
   ],
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "# Predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Accuracy\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "accuracy = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "# Precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "# Recall\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "# F1 Score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "# AUC\n",
    "auc_eval = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = auc_eval.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f1253d-4f22-4e04-b8b6-21343e0395e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n|label|prediction|count|\n+-----+----------+-----+\n|  0.0|       0.0|   71|\n|  0.0|       1.0|    7|\n|  1.0|       0.0|   21|\n|  1.0|       1.0|   70|\n+-----+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Group by label and prediction\n",
    "conf_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "conf_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632861b9-6f13-44f9-88fc-3600ed5a176a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "Model used: Logistic Regression\n",
    "\n",
    "Accuracy, Precision, Recall, F1 Score obtained"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HeartDiseaseClassificationML",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}